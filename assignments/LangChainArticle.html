<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="../index.css">
    <link rel="stylesheet" href="../prism.css">
    <title>PyCharm Getting Started</title>
</head>
<body>
    
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <a  class="navbar-brand" href="../index.html">
            <img class="logo" src="../img/logo.png" alt="" >
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav mr-auto">
            <li class="nav-item active">
              <a class="nav-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Assignments
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="#">LangChain Article</a>
                <a class="dropdown-item" href="./TechnicalDescription.html">Technical Description</a>
              </div>
            </li>
            
          </ul>
  
        </div>
      </nav>

      <main>
        <div class="container">
            <div class="text-start">
                <img class="img-fluid mt-3" src="../img/banner.png">
            </div>
            <h1 class="display-5 text-start">Tailoring AI Responses with LangChain: A Guide to Custom Language Models Using Web-Scraped Data </h1>
        </div>
        
        <div class="container">
    
            <div>
                <p class="text-start author">By <a class="name" href="https://ileka2468.github.io/website/">William Ileka</a>&nbsp&nbsp|&nbsp&nbspNovember 10, 2023</p>
            </div>
            
        </div>

        <div class="container">
            <p>
              With the rapid development of the artificial intelligence (AI) industry, the ability to adapt language models to individual needs is increasingly important. Fortunately this is now easily achievable with open source options such as the library provided by LangChain, an AI framework that combines a wide range of language model tools into a unified software development kit, or at least that's how they describe it. This guide will delve into the process of setting up a custom language model using LangChain, focusing specifically on using web-scraped data as a resource to guide the model's responses. So whether you're a data scientist, an AI geek, or a developer looking to integrate AI into your app, this article will show you the steps and concepts you need to use LangChain with your data. This guide will use a practical example that demonstrates how web scraped curriculum information obtained from the DePaul University website can be integrated into a conversational AI model. By the end of this tutorial, you'll have an AI model ready to answer questions about your dataset, happy chatting!
            </p>
        </div>

        <div class="container mt-5">
          <div class="row">
              <div class="col-md-12">
                  <h2 class="mb-4">LangChain Setup Prerequisites</h2>
                  <div id="accordion">
                      <!-- LangChain Installation -->
                      <div class="card">
                          <div class="card-header" id="headingOne">
                              <h5 class="mb-0">
                                  <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
                                    <span class="numbering">1.</span> Installing LangChain
                                  </button>
                              </h5>
                          </div>
  
                          <div id="collapseOne" class="collapse show" aria-labelledby="headingOne" data-parent="#accordion">
                              <div class="card-body">
                                  <p>LangChain requires Python 3.8.1 or newer. Install LangChain using pip:</p>
                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install langchain
                                    </code>
                                  </pre>
                                  <p>Make sure Python and pip are correctly installed on your system before proceeding.</p>
                              </div>
                          </div>
                      </div>
                      <!-- OpenAI API Key -->
                      <div class="card">
                          <div class="card-header" id="headingTwo">
                              <h5 class="mb-0">
                                  <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                                    <span class="numbering">2.</span> Obtaining Your OpenAI API Key
                                  </button>
                              </h5>
                          </div>
                          <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordion">
                              <div class="card-body">
                                  <p>To use OpenAI's models, you need an API key. Follow these steps:</p>
                                  <ul>
                                      <li>Sign up or log in to your OpenAI account.</li>
                                      <li>Navigate to the API section and follow the prompts to create a new API key.</li>
                                      <li>Keep your API key secure; it's your personal access token to OpenAI's services.</li>
                                      <li>Note: OpenAI's service is NOT free. Following this tutorial will incur your account usage based charges. For more information on rates and billing visit their website at <a href="https://openai.com/pricing">https://openai.com/pricing</a>  </li>
                                  </ul>
                              </div>
                          </div>
                      </div>
                      <!-- Additional Dependencies -->
                      <div class="card">
                          <div class="card-header" id="headingThree">
                              <h5 class="mb-0">
                                  <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                                     <span class="numbering">3.</span> Installing Additional Dependencies
                                  </button>
                              </h5>
                          </div>
                          <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordion">
                              <div class="card-body">
                           
                                  <p>Depending on your project's requirements, you might need additional dependencies like selenium, bs4 or requests. Install them using pip:</p>
                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install selenium
                                    </code>
                                  </pre>

                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install bs4
                                    </code>
                                  </pre>

                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install requests
                                    </code>
                                  </pre>
                              </div>
                          </div>
                      </div>
                  </div>
              </div>
          </div>
      </div>



      <div class="container mt-5">
        <h2>Data Collection and Organization for AI</h2>
        <p class="lead">The first step to creating an accurate language model is collecting and organizing our data. If you have random and irrelevant text in your documents the language model can make up details and cause random chat abnormalities.</p>

        <!-- Step 1: Data Determination -->
        <div class="card mb-3">
            <div class="card-header">
                Step 1: Determine the Data You Need
            </div>
            <div class="card-body">
                <p>Identify the data you need for your project. Start by finding a sitemap for your target website. If a sitemap doesn’t exist, create one using online tools such as <a href="https://www.xml-sitemaps.com/">https://www.xml-sitemaps.com/</a>. Remember to always respect the website’s robots.txt file to ensure that web scraping is allowed. In this guide we have identified the website site map at <a target="_blank" href="https://wwwdev.cdm.depaul.edu/sitemap.xml">https://wwwdev.cdm.depaul.edu/sitemap.xml</a>.</p>
                 <div> 
                  <p>We can use the following python code on the sitemap to extract the urls to a text file and save them for later.</p>
                  <pre>
                    <code class="language-python">
                      from bs4 import BeautifulSoup

                      sitemap_path = 'sitemap.xml'
                      with open(sitemap_path, 'r', encoding='utf-8') as file:
                          sitemap_xml = file.read()

                      soup = BeautifulSoup(sitemap_xml, 'xml')  # Parse XML with BeautifulSoup
                      urls = [url_tag.text for url_tag in soup.find_all('loc')]  # Extract URLs from <loc> tags

                      with open("urls.txt", "w") as f:
                          for url in urls:
                              f.write(url)
                    </code>
                  </pre>
                </div>
            </div>
        </div>

        <!-- Step 2: Choosing the Right Tool -->
        <div class="card mb-3">
            <div class="card-header">
                Step 2: Choose the Right Scraping Tool
            </div>
            <div class="card-body">
                <p>Now we can write some scraping code to extract the web page data using the urls.txt file we generated in the previous step. Depending on whether you are using selenium or bs4 you should save the scraped data as .txt files and put them in a folder. It is advised to store your scraped data in the file in a formatted way such as JSON, XML, or some consistent custom format.</p>


                <div>
                  <p>
                    Alternatively, if you want your model to be able to refer to source documents and provide citations, you should also save the link to the webpage in your text file, save it on the first line or prefix it with a unique identifier so that it can be programmatically extracted from the text file and used in code.
                  </p>
                  
                </div>
                <div>
                  <p>In our example we will simply just extract the page content and major info such as the major name and concentration. This will be included in the file to help make searching easier for the model.</p> 

                  <pre>
                    <code class="language-python">
                      import bs4
                      import requests

                      links = []
                      with open('urls.txt' "r") as f:
                          for line in f:
                              links.append(line)

                      for link in links:
                          response = requests.get(link)

                          # Check if the request was successful
                          if response.status_code == 200:
                              # Step 2: Parse HTML content
                              soup = bs4.BeautifulSoup(response.text, 'html.parser')

                              concentration = soup.find('h2', id="programPageConcentration")
                              major = soup.find('h1', class_="PageTitle-v02")
                              final_name = None
                              if concentration:
                                  final_name = f"{major.text} - {concentration.text}"
                              else:
                                  final_name = f"{major.text} - General Track"

                              target_element = soup.find('div', class_='pageContent')

                              with open(f'majordata/{final_name}.txt', 'w') as f:
                                  f.write(f"Depaul University - Jarvis College of Computing and Digital Media\nMajor Name - {final_name}\nMajor Data Below:\n\n{target_element.text}")

                    </code>
                  </pre>
                </div>
            </div>
        </div>

        <!-- Step 3: Running the Scraper -->
        <div class="card mb-3">
            <div class="card-header">
                Step 3: Run Your Scraper
            </div>
            <div class="card-body">
                <p>Execute your scraping code to download the required webpage data. Store the data in a text format within a designated folder, as these files will be used later to create document objects for the AI model. I have saved my files into a folder called majordata depending on your scraping criteria you may have more or less text files than me, the amount of files is dependent on the amount of data present on the website you are scraping and what exactly you decided to pull from it. In any event, before proceeding to the next step make sure to have a directory of files and inspect them to confirm there were no errors in your code and you extracted the information you wanted.</p>

                <div class="text-center">
                  <img class="img-fluid graph" src="../img/files.PNG" alt="">
                </div>
            </div>
        </div>

    </div>

    <div class="container mt-5">
      <h2>Understanding the AI Workflow</h2>
      <p class="lead">This section provides a high-level overview of the workflow from web-scraped content to the final AI response. Understanding each component is crucial for effectively tailoring AI responses.</p>

      <div class="text-center mb-3">
        <img class="img-fluid workflowpic" src="../img/graphics.png">
      </div>

      <!-- Workflow Steps -->
      <div class="row">
          <!-- Step 1: Web Scraped Content -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Web Scraped Content</div>
                  <div class="card-body">
                      <p>Initially, we collect relevant data from the web. This raw content is the foundational data that will be processed and used by the AI model.</p>
                  </div>
              </div>
          </div>

          <!-- Step 2: Embedding Model -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Embedding Model</div>
                  <div class="card-body">
                      <p>The embedding model transforms the scraped content into a numerical form, known as embeddings. These embeddings represent the content in a way that's understandable to AI.</p>
                  </div>
              </div>
          </div>

          <!-- Step 3: Document Embeddings -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Document Embeddings</div>
                  <div class="card-body">
                      <p>Document embeddings are the numerical representations of our original content, now ready for further processing and retrieval by the AI.</p>
                  </div>
              </div>
          </div>
      </div>

      <div class="row">
          <!-- Step 4: Vector Store -->
          <div class="col-md-6 mb-4">
              <div class="card h-100">
                  <div class="card-header">Vector Store</div>
                  <div class="card-body">
                      <p>The vector store is a database where document embeddings are stored. It allows for efficient searching and retrieval of relevant information in response to queries.</p>
                  </div>
              </div>
          </div>

          <!-- Step 5: Query Processing -->
          <div class="col-md-6 mb-4">
              <div class="card h-100">
                  <div class="card-header">Query Processing</div>
                  <div class="card-body">
                      <p>A user's query is also transformed into embeddings, allowing the AI to understand and match it with the most relevant information from the vector store.</p>
                  </div>
              </div>
          </div>
      </div>

      <div class="row">
          <!-- Step 6: Summarization Model -->
          <div class="col-md-6 mb-4 offset-md-3">
              <div class="card h-100">
                  <div class="card-header">Summarization Model</div>
                  <div class="card-body">
                      <p>Finally, the summarization model takes the retrieved information and generates a concise, relevant response to the user's query.</p>
                  </div>
              </div>
          </div>
      </div>

  </div>



  <div class="container mt-5">
    <h2>Setting up the Embedded Language Model</h2>
    <p class="lead">Now that we have our scraped content, we can create the documents from them and pass it to the document embedding model. LangChain has many embedding models such as OpenAI Embeddings, HuggingFace Embeddings and more. This article will be using OpenAI Embeddings since we are using their API. However, it's worth noting that Hugging Face provides a free alternative to OpenAI. Checkout LangChain's documentation as it offers valuable insights to help you make an informed decision about the most suitable embedding model for your needs.</p>

    <!-- Step 1: Data Determination -->
    <div class="card mb-3">
        <div class="card-header">
            Step 1: Import the Required Modules
        </div>
        <div class="card-body">
            <p>In order to create the documents will be importing some modules from LangChain and Python:</p>
             <div> 
              
              <pre>
                <code class="language-python">
                  import os
                  import pickle
                  from dotenv import load_dotenv
                  from langchain.vectorstores import FAISS
                  from langchain.embeddings import OpenAIEmbeddings
                  from langchain.docstore.document import Document
                </code>
              </pre>
            </div>
        </div>
    </div>

    <!-- Step 2: Choosing the Right Tool -->
    <div class="card mb-3">
        <div class="card-header">
            Step 2: Setting your API key
        </div>
        <div class="card-body">
            <p>Now that we are working with the API we need to include our API key as an environment variable so any requests made by LangChain can be authenticated. Never hardcode your API key into your code rather use a .env file. </p>
            <div>
              <ol>
                <li> Make a .env file in the root of your project directory and add your API key.
                  <div>
                    <pre>
                      <code class="language-">
                        API_KEY = &lt;your-api-key&gt;
                      </code>
                    </pre>
                  </div>
                </li>

                <li> In your python script with the imported modules from step 1, load your API key from the .env file.
                  <div>
                    <pre>
                      <code class="language-python">
                        from dotenv import load_dotenv
                        import os
                        import pickle
                        from dotenv import load_dotenv
                        from langchain.vectorstores import FAISS
                        from langchain.embeddings import OpenAIEmbeddings
                        from langchain.docstore.document import Document

                        load_dotenv()
                      </code>
                    </pre>
                  </div>
                </li>
              </ol>
              
            </div>
        </div>
    </div>

    <!-- Step 3: Running the Scraper -->
    <div class="card mb-3">
        <div class="card-header">
            Step 3: Creating the documents and feeding them to the embedding model.
        </div>
        <div class="card-body">
            <p>Take note of the folder name that you saved your web scraped content to because in this step we will be looping through that folder and all of its contents and creating the documents. This step will take some fiddling and trial and error as depending on the size of an individual document you may have to further split it into two different documents.</p>

            <div>
              <p>First we will create a list of all the files in the folder and then loop through them and create the documents.</p> 

              <pre>
                <code class="language-python">
                  # Create a list of all the files in the folder
                  files = os.listdir('./majordata')

                  # Loop through the files and create the documents
                  documents = []

                  for file in files:
                    with open(os.path.join(path, file), 'r') as f:
                      docs.append(Document(page_content=f.read(), metadata={"source": 1}))

                  '''
                  The metadata is optional but it is useful if you want to know the source document that the AI is referencing.
                  Ideally you would put the link of the website that the document is from instead of 1.
                  '''
                </code>
              </pre>
        </div>

        <div>
          <p>In the event that a single file is too large to fit in a single document object, or you find that your conversational model is lacking some details or context, try splitting your files into two separate documents. However doing this causes a new problem, say the AI finds a document that matches its similarity. If the answer it found overflows into a second document it will be missing the rest of the context. So in our solution we must also include what is called a chunk overlap which defines how much text from a previous document will be included in the next document to prevent missing context.</p>

          <pre>
            <code class="language-python">
              import os

              # Define the maximum size of a document and the size of the overlap
              max_size = 10000  # Adjust this value as needed
              overlap_size = 1000  # Adjust this value as needed
              
              # Create a list of all the files in the folder
              files = os.listdir('./majordata')
              
              # Loop through the files and create the documents
              documents = []
              
              for file in files:
                  with open(os.path.join(path, file), 'r') as f:
                      content = f.read()
                      
                  # If the file is too large, split it into chunks with overlap
                  if len(content) > max_size:
                      start = 0
                      while start < len(content):
                          end = min(start + max_size, len(content))
                          chunk = content[start:end]
                          documents.append(Document(page_content=chunk, metadata={"source": 1}))
                          start = end - overlap_size
                  else:
                      documents.append(Document(page_content=content, metadata={"source": 1}))
            </code>
          </pre>
          <p>Ultimately, depending on your project needs you can choose the first simple script or the second one. I will be using the first script as my content files are not large enough to necessitate splitting them further.</p>
        </div>
    </div>

</div>

      
</main>
      <footer class="bg-light text-center text-lg-start">
        <!-- Copyright -->
        <div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0.2);">
          © 2023 Copyright:
          <a class="text-dark" href="https://ileka2468.github.io/website">William Ileka</a>
        </div>
      </footer>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script type="module" src="../prism.js"></script>
</body>
</html>