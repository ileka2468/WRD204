<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="../index.css">
    <link rel="stylesheet" href="../prism.css">
    <title>LangChain using Web-Scraped data</title>
</head>
<body>
    
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <a  class="navbar-brand" href="../index.html">
            <img class="logo" src="../img/logo.png" alt="" >
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav mr-auto">
            <li class="nav-item active">
              <a class="nav-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Assignments
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                <a class="dropdown-item" href="#">LangChain Article</a>
                <a class="dropdown-item" href="./TechnicalDescription.html">Technical Description</a>
              </div>
            </li>
            
          </ul>
  
        </div>
      </nav>

      <main>
        <div class="container">
            <div class="text-start">
                <img class="img-fluid mt-3" src="../img/banner.png">
            </div>
            <h1 class="display-5 text-start">Tailoring AI Responses with LangChain: A Guide to Custom Language Models Using Web-Scraped Data </h1>
        </div>
        
        <div class="container">
    
            <div>
                <p class="text-start author">By <a class="name" href="https://ileka2468.github.io/website/">William Ileka</a>&nbsp&nbsp|&nbsp&nbspNovember 10, 2023</p>
            </div>
            
        </div>

        <div class="container">
            <p>
              With the rapid development of the artificial intelligence (AI) industry, the ability to adapt language models to individual needs is increasingly important. Fortunately, this is now easily achievable with open source options such as the library provided by LangChain, an AI framework that combines a wide range of language model tools into a unified software development kit, or at least that's how they describe it. This guide will delve into the process of setting up a custom language model using LangChain, focusing specifically on using web-scraped data as a resource to guide the model's responses. So whether you're a data scientist, an AI geek, or a developer looking to integrate AI into your app, this article will show you the steps and concepts you need to use LangChain with your data. This guide will use a practical example that demonstrates how web scraped curriculum information obtained from the DePaul University website can be integrated into a conversational AI model. By the end of this tutorial, you'll have an AI model ready to answer questions about your dataset, happy chatting!
            </p>
        </div>

        <div class="container mt-5">
          <div class="row">
              <div class="col-md-12">
                  <h2 class="mb-4">LangChain Setup Prerequisites</h2>
                  <div id="accordion">
                      <!-- LangChain Installation -->
                      <div class="card">
                          <div class="card-header" id="headingOne">
                              <h5 class="mb-0">
                                  <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
                                    <span class="numbering">1.</span> Installing LangChain
                                  </button>
                              </h5>
                          </div>
  
                          <div id="collapseOne" class="collapse show" aria-labelledby="headingOne" data-parent="#accordion">
                              <div class="card-body">
                                  <p>LangChain requires Python 3.8.1 or newer. Install LangChain using pip:</p>
                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install langchain
                                    </code>
                                  </pre>
                                  <p>Make sure Python and pip are correctly installed on your system before proceeding.</p>
                              </div>
                          </div>
                      </div>
                      <!-- OpenAI API Key -->
                      <div class="card">
                          <div class="card-header" id="headingTwo">
                              <h5 class="mb-0">
                                  <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                                    <span class="numbering">2.</span> Obtaining Your OpenAI API Key
                                  </button>
                              </h5>
                          </div>
                          <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordion">
                              <div class="card-body">
                                  <p>To use OpenAI's models, you need an API key. Follow these steps:</p>
                                  <ul>
                                      <li>Sign up or log in to your OpenAI account.</li>
                                      <li>Navigate to the API section and follow the prompts to create a new API key.</li>
                                      <li>Keep your API key secure; it's your personal access token to OpenAI's services.</li>
                                      <li>Note: OpenAI's service is NOT free. Following this tutorial will incur your account usage based charges. For more information on rates and billing visit their website at <a href="https://openai.com/pricing">https://openai.com/pricing</a>  </li>
                                  </ul>
                              </div>
                          </div>
                      </div>
                      <!-- Additional Dependencies -->
                      <div class="card">
                          <div class="card-header" id="headingThree">
                              <h5 class="mb-0">
                                  <button class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                                     <span class="numbering">3.</span> Installing Additional Dependencies
                                  </button>
                              </h5>
                          </div>
                          <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordion">
                              <div class="card-body">
                           
                                  <p>Depending on your project's requirements, you might need additional dependencies like selenium, bs4 or requests. Install them using pip:</p>
                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install selenium
                                    </code>
                                  </pre>

                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install bs4
                                    </code>
                                  </pre>

                                  <pre>
                                    <code class="language-PowerShell">
                                      pip install requests
                                    </code>
                                  </pre>
                              </div>
                          </div>
                      </div>
                  </div>
              </div>
          </div>
      </div>



      <div class="container mt-5">
        <h2>Data Collection and Organization for AI</h2>
        <p class="lead">The first step to creating an accurate language model is collecting and organizing our data. If you have random and irrelevant text in your documents the language model can make up details and cause random chat abnormalities.</p>

        <!-- Step 1: Data Determination -->
        <div class="card mb-3">
            <div class="card-header">
                Step 1: Determine the Data You Need
            </div>
            <div class="card-body">
                <p>Identify the data you need for your project. Start by finding a sitemap for your target website. If a sitemap doesn’t exist, create one using online tools such as <a href="https://www.xml-sitemaps.com/">https://www.xml-sitemaps.com/</a>. Remember to always respect the website’s robots.txt file to ensure that web scraping is allowed. In this guide we have identified the website site map at <a target="_blank" href="https://wwwdev.cdm.depaul.edu/sitemap.xml">https://wwwdev.cdm.depaul.edu/sitemap.xml</a>.</p>
                 <div> 
                  <p>We can use the following python code on the sitemap to extract the urls to a text file and save them for later.</p>
                  <pre>
                    <code class="language-python">
                      from bs4 import BeautifulSoup

                      sitemap_path = 'sitemap.xml'
                      with open(sitemap_path, 'r', encoding='utf-8') as file:
                          sitemap_xml = file.read()

                      soup = BeautifulSoup(sitemap_xml, 'xml')  # Parse XML with BeautifulSoup
                      urls = [url_tag.text for url_tag in soup.find_all('loc')]  # Extract URLs from <loc> tags

                      with open("urls.txt", "w") as f:
                          for url in urls:
                              f.write(url)
                    </code>
                  </pre>
                </div>
            </div>
        </div>

        <!-- Step 2: Choosing the Right Tool -->
        <div class="card mb-3">
            <div class="card-header">
                Step 2: Choose the Right Scraping Tool
            </div>
            <div class="card-body">
                <p>Now we can write some scraping code to extract the web page data using the urls.txt file we generated in the previous step. Depending on whether you are using selenium or bs4 you should save the scraped data as .txt files and put them in a folder. It is advised to store your scraped data in the file in a formatted way such as JSON, XML, or some consistent custom format.</p>


                <div>
                  <p>
                    Alternatively, if you want your model to be able to refer to source documents and provide citations, you should also save the link to the webpage in your text file, save it on the first line or prefix it with a unique identifier so that it can be programmatically extracted from the text file and used in code.
                  </p>
                  
                </div>
                <div>
                  <p>In our example we will simply just extract the page content and major info such as the major name and concentration. This will be included in the file to help make searching easier for the model.</p> 

                  <pre>
                    <code class="language-python">
                      import bs4
                      import requests

                      links = []
                      with open('urls.txt' "r") as f:
                          for line in f:
                              links.append(line)

                      for link in links:
                          response = requests.get(link)

                          # Check if the request was successful
                          if response.status_code == 200:
                              # Step 2: Parse HTML content
                              soup = bs4.BeautifulSoup(response.text, 'html.parser')

                              concentration = soup.find('h2', id="programPageConcentration")
                              major = soup.find('h1', class_="PageTitle-v02")
                              final_name = None
                              if concentration:
                                  final_name = f"{major.text} - {concentration.text}"
                              else:
                                  final_name = f"{major.text} - General Track"

                              target_element = soup.find('div', class_='pageContent')

                              with open(f'majordata/{final_name}.txt', 'w') as f:
                                  f.write(f"DePaul University - Jarvis College of Computing and Digital Media\nMajor Name - {final_name}\nMajor Data Below:\n\n{target_element.text}")

                    </code>
                  </pre>
                </div>
            </div>
        </div>

        <!-- Step 3: Running the Scraper -->
        <div class="card mb-3">
            <div class="card-header">
                Step 3: Run Your Scraper
            </div>
            <div class="card-body">
                <p>Execute your scraping code to download the required webpage data. Store the data in a text format within a designated folder, as these files will be used later to create document objects for the AI model. I have saved my files into a folder called majordata depending on your scraping criteria you may have more or less text files than me, the amount of files is dependent on the amount of data present on the website you are scraping and what exactly you decided to pull from it. In any event, before proceeding to the next step make sure to have a directory of files and inspect them to confirm there were no errors in your code and you extracted the information you wanted.</p>

                <div class="text-center">
                  <img class="img-fluid graph" src="../img/files.PNG" alt="">
                </div>
            </div>
        </div>

    </div>

    <div class="container mt-5">
      <h2>Understanding the AI Workflow</h2>
      <p class="lead">This section provides a high-level overview of the workflow from web-scraped content to the final AI response. Understanding each component is crucial for effectively tailoring AI responses.</p>

      <div class="text-center mb-3">
        <img class="img-fluid workflowpic" src="../img/graphics.png">
      </div>

      <!-- Workflow Steps -->
      <div class="row">
          <!-- Step 1: Web Scraped Content -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Web Scraped Content</div>
                  <div class="card-body">
                      <p>Initially, we collect relevant data from the web. This raw content is the foundational data that will be processed and used by the AI model.</p>
                  </div>
              </div>
          </div>

          <!-- Step 2: Embedding Model -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Embedding Model</div>
                  <div class="card-body">
                      <p>The embedding model transforms the scraped content into a numerical form, known as embeddings. These embeddings represent the content in a way that's understandable to AI.</p>
                  </div>
              </div>
          </div>

          <!-- Step 3: Document Embeddings -->
          <div class="col-md-4 mb-4">
              <div class="card h-100">
                  <div class="card-header">Document Embeddings</div>
                  <div class="card-body">
                      <p>Document embeddings are the numerical representations of our original content, now ready for further processing and retrieval by the AI.</p>
                  </div>
              </div>
          </div>
      </div>

      <div class="row">
          <!-- Step 4: Vector Store -->
          <div class="col-md-6 mb-4">
              <div class="card h-100">
                  <div class="card-header">Vector Store</div>
                  <div class="card-body">
                      <p>The vector store is a database where document embeddings are stored. It allows for efficient searching and retrieval of relevant information in response to queries.</p>
                  </div>
              </div>
          </div>

          <!-- Step 5: Query Processing -->
          <div class="col-md-6 mb-4">
              <div class="card h-100">
                  <div class="card-header">Query Processing</div>
                  <div class="card-body">
                      <p>A user's query is also transformed into embeddings, allowing the AI to understand and match it with the most relevant information from the vector store.</p>
                  </div>
              </div>
          </div>
      </div>

      <div class="row">
          <!-- Step 6: Summarization Model -->
          <div class="col-md-6 mb-4 offset-md-3">
              <div class="card h-100">
                  <div class="card-header">Summarization Model</div>
                  <div class="card-body">
                      <p>Finally, the summarization model takes the retrieved information and generates a concise, relevant response to the user's query.</p>
                  </div>
              </div>
          </div>
      </div>

  </div>



  <div class="container mt-5">
    <h2>Setting up the Embedded Language Model</h2>
    <p class="lead">Now that we have our scraped content, we can create the documents from them and pass it to the document embedding model. LangChain has many embedding models such as OpenAI Embeddings, HuggingFace Embeddings and more. This article will be using OpenAI Embeddings since we are using their API. However, it's worth noting that Hugging Face provides a free alternative to OpenAI. Checkout LangChain's documentation as it offers valuable insights to help you make an informed decision about the most suitable embedding model for your needs.</p>

    <!-- Step 1: Data Determination -->
    <div class="card mb-3">
        <div class="card-header">
            Step 1: Import the Required Modules
        </div>
        <div class="card-body">
            <p>In order to create the documents will be importing some modules from LangChain and Python:</p>
             <div> 
              
              <pre>
                <code class="language-python">
                  import os
                  import pickle
                  from dotenv import load_dotenv
                  from langchain.vectorstores import FAISS
                  from langchain.embeddings import OpenAIEmbeddings
                  from langchain.docstore.document import Document
                </code>
              </pre>
            </div>
        </div>
    </div>

    <!-- Step 2: Choosing the Right Tool -->
    <div class="card mb-3">
        <div class="card-header">
            Step 2: Setting your API key
        </div>
        <div class="card-body">
            <p>Now that we are working with the API we need to include our API key as an environment variable so any requests made by LangChain can be authenticated. Never hardcode your API key into your code rather use a .env file. </p>
            <div>
              <ol>
                <li> Make a .env file in the root of your project directory and add your API key.
                  <div>
                    <pre>
                      <code class="language-">
                        API_KEY = &lt;your-api-key&gt;
                      </code>
                    </pre>
                  </div>
                </li>

                <li> In your python script with the imported modules from step 1, load your API key from the .env file.
                  <div>
                    <pre>
                      <code class="language-python">
                        from dotenv import load_dotenv
                        import os
                        import pickle
                        from dotenv import load_dotenv
                        from langchain.vectorstores import FAISS
                        from langchain.embeddings import OpenAIEmbeddings
                        from langchain.docstore.document import Document

                        load_dotenv()
                      </code>
                    </pre>
                  </div>
                </li>
              </ol>
              
            </div>
        </div>
    </div>

    <!-- Step 3: Running the Scraper -->
    <div class="card mb-3">
        <div class="card-header">
            Step 3: Creating the documents.
        </div>
        <div class="card-body">
            <p>Take note of the folder name that you saved your web scraped content to because in this step we will be looping through that folder and all of its contents and creating the documents. This step will take some fiddling and trial and error as depending on the size of an individual document you may have to further split it into two different documents.</p>

            <div>
              <p>First we will create a list of all the files in the folder and then loop through them and create the documents.</p> 

              <pre>
                <code class="language-python">
                  # Create a list of all the files in the folder
                  files = os.listdir('./majordata')

                  # Loop through the files and create the documents
                  path = './majordata' # Change this to the path of your folder
                  documents = []

                  for file in files:
                    with open(os.path.join(path, file), 'r') as f:
                      content = f.read()
                      source = content[content.find(">") + 3 : content.find("aspx") + 4]
                      docs.append(Document(page_content=content, metadata={"source": source}))

                  '''
                  The metadata is optional but it is useful if you want to know the source document that the AI is referencing.
                  If you don't want sources just delete the metadata parameter and the source variable above it.
                  '''
                </code>
              </pre>
        </div>

        <div>
          <p>In the event that a single file is too large to fit in a single document object, or you find that your conversational model is lacking some details or context, try splitting your files into two separate documents. However doing this causes a new problem, say the AI finds a document that matches its similarity. If the answer it found overflows into a second document it will be missing the rest of the context. So in our solution we must also include what is called a chunk overlap which defines how much text from a previous document will be included in the next document to prevent missing context.</p>

          <pre>
            <code class="language-python">
              import os

              # Define the maximum size of a document and the size of the overlap
              max_size = 10000  # Adjust this value as needed
              overlap_size = 1000  # Adjust this value as needed
              
              # Create a list of all the files in the folder
              files = os.listdir('./majordata')
              
              # Loop through the files and create the documents
              path = './majordata' # Change this to the path of your folder
              documents = []
              
              for file in files:
                  with open(os.path.join(path, file), 'r') as f:
                      content = f.read()
                      source = content[content.find(">") + 3 : content.find("aspx") + 4]
              
                  # If the file is too large, split it into chunks with overlap
                  if len(content) > max_size:
                      start = 0
                      while start < len(content):
                          end = min(start + max_size, len(content))
                          chunk = content[start:end]
                          
                          # If the chunk is still too large, split it further
                          while len(chunk) > max_size:
                              sub_end = min(len(chunk), max_size)
                              sub_chunk = chunk[:sub_end]
                              documents.append(Document(page_content=sub_chunk, metadata={"source": source}))
                              chunk = chunk[sub_end - overlap_size:]
                          
                          documents.append(Document(page_content=chunk, metadata={"source": source}))
                          start = end - overlap_size
                  else:
                      documents.append(Document(page_content=content, metadata={"source": source}))
            </code>
          </pre>
          <p>Ultimately, depending on your project needs you should choose the script that fits your data the best. I will be using the first script as my content files are not large enough to necessitate splitting them further.</p>
        </div>

        

        
    </div>

</div>

<div class="card mb-3">
  <div class="card-header">
      Step 4: Creating and saving the vector store.
  </div>
  <div class="card-body">
      <p>Now that we have our documents stored in a list we can create our vector store and save it to the local disk using the pickle library. After executing this code you should see a .pkl file in the current working directory.</p>
      <div>
        <pre>
          <code class="language-python">
            # select the embedding model
            embeddings = OpenAIEmbeddings()

            # create the vector store
            db = FAISS.from_documents(documents, embeddings)

            # save the vector store
            with open("storepickl.pkl", "wb") as f:
                pickle.dump(db, f)
          </code>
        </pre>
      </div>
  </div>
</div>



<div class="container mt-5">
  <h2>Querying an LLM with our data</h2>
  <p class="lead">The next step is to load our vector store and pass it to an LM so it can give us an answer based off our context.</p>

  <div class="card mb-3">
      <div class="card-header">
          Step 1: Load the serialized vector store.
      </div>
      <div class="card-body">
          <p>Similarly to how we save the vector store, before querying it we need to load it into the program. To do this we will be using the pickle module again.</p>

          <pre>
            <code class="language-python">
              # Load the serialized vector store
              with open('vector_store.pkl', 'rb') as f:
                  vector_store = pickle.load(f)
    
            </code>
          </pre>
      </div>

      
  </div>


  <div class="card mb-3">
      <div class="card-header">
          Step 2: Instantiate an LLM and choose the model.
      </div>
      <div class="card-body">
          <p>In this step we will choose our LM, since I have been using OpenAI this entire example I will be using the OpenAI LM. There are two parameters we should supply, the first one is temperature. Temperature is a measure of how random, creative, and confident the generated text is. A higher temperature will lead to more creative text that may not be as factual, while a lower temperature will lead to a more factual and focused answer. Choose an appropriate temperature for your application, I am making an academic advisor so I will opt for a low temperature.</p>

          <div>
            <p>
              The second parameter is the model name. This is the name of the model that will be used to generate the response. A list of available models can be found on the OpenAI <a href="https://platform.openai.com/docs/modelshttps://platform.openai.com/docs/models">website</a> . For this example I will be using the new 'gpt-3.5-turbo-1106' model with 16k context.
            </p>

            <p>
              Context is the amount of tokens the model can take in as input. The more context the model has the more accurate the response will be. It will also be able to deal with longer queries. However, the more context the model has the longer it will take to generate a response. The context is measured in tokens, for reference the gpt-3.5 model which is free to use has a 4096 token context window, which is is equivalent to about 8,000 words.
            </p>

            <pre>
              <code class="language-python">
                llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-1106')
              </code>
            </pre>
            
          </div>
      </div>

  </div>


  <div class="card mb-3">
      <div class="card-header">
          Step 3: Select a retrieval method.
      </div>
      <div class="card-body">
        <p>LangChain has several retrieval method chains, a retrieval chain is a series of steps that are taken in the process of retrieving the answer from the LM. For example, there are retrieval chains for just answers and retrieval chains for answers along with sources. Choose the appropriate retrieval method based off whether you added source metadata to your documents.</p>

        <div>
          <p>
            For this example I will be using the 'RetrievalQAWithSourcesChain' retrieval method as I added source metadata to my documents.
          </p>

          <pre>
            <code class="language-python">
              # we pass in as kwargs the vector store and the language model
              chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vector_store.as_retriever())
            </code>
          </pre>
      </div>
  </div>

  <div class="card mb-3">
    <div class="card-header">
        Step 3: Run the query and get a response.
    </div>
    <div class="card-body">
      <p>To query the chain we simple supply a dictionary with the key of "question" and pass in the prompt as the value.</p>

      <p>The prompt should fit your specific needs, for my example I will be passing a contextual prompt that tells the AI how to act and structure its response, and then the actual question.</p>
      <pre>
        <code class="language-python">

          question = "What does CDM stand for? And what classes do the CS Software Dev concentration majors take in their first year?"
          query = f"You are an automated Academic Advisor at DePaul university called demon bot, you will be asked Course/Major specific questions about CDM majors. Make sure to give detailed responses that answer the core of the question accurately. If you don't know the answer or are unsure of it refer students to https://www.cdm.depaul.edu/ and tell them to speak to their advisor. Make sure to include any Liberal study requirements courses for questions that pertain to what classes a student might take, be very specific and do not forget any classes. Student question: {question}"

          answer = chain({"question": query})
          print(dict(answer))
        </code>
      </pre>

      <p>After some seconds you should see a response in the terminal:</p>

      <pre>
        <code class="language-PowerShell long">
          {'question': "You are an automated Academic Advisor at DePaul university called demon bot, you will be asked Course/Major specific questions about CDM majors. Make sure to give detailed responses that answer the core of the question accurately. If you don't know the answer or are unsure of it refer students to https://www.cdm.depaul.edu/ and tell them to speak to their advisor. DO NOT FORGET to list the required Liberal Studies requirements courses for questions that pertain to what classes a student might take. Be very specific and do not forget any classes. Most importantly do not hallucinate details. Student question: What classes do Computer Science students who are in the Software Development concentration take in their second year?", 'answer': "Computer Science students who are in the Software Development concentration take the following classes in their second year:\n- CSC 299 Sophomore Lab in Applied Computing\n- CSC 301 Data Structures II\n- CSC 321 Design and Analysis of Algorithms\n- CSC 347 Concepts of Programming Languages\n- CSC 373 Computer Systems I\n- CSC 374 Computer Systems II\n- WRD 204 Technical Writing\nLiberal Studies Requirements:\n- LSP 200 Seminar on Race, Power, and Resistance\n- CSC 208 Ethics in Technology\n- Religious Dimensions\n- Social, Cultural, and Behavioral Inquiry\n- Historical Inquiry\n", 'sources': "https://www.cdm.depaul.edu/academics/Pages/Current/Requirements-BS-In-Computer-Science-Software-Development.aspx"}
        </code>
      </pre>

      <p>Since I did the RetrievalQAWithSourcesChain in the response I will see the question, answer, and sources. Epic!</p>
      <p>Any of these values can be singled out by using the Python dictionary notation to retrieve a single value.</p>
      <pre>
        <code class="language-python">
          answer = chain({"question": query})
          print(dict(answer)['answer']) # print answer, or sources, or question.
        </code>
      </pre>

      <p>
        You'll notice the raw dictionary has escape sequences, this is for formatting purposes and will disappear when you display it in some standard output.
      </p>

      <div class="text-center">
        <img class="img-fluid" src="../img/terminal.PNG" alt="">
      </div>

    </div>
</div>
<div class="container mt-6">
  <div class="row">
    <!-- Embedded LangChain Model -->
    <div class="col-md-6">
      <h2>Embedded LangChain Model</h2>
      <p>
        The embedded LangChain model leverages contextual information from custom data sources, providing tailored and nuanced responses. This approach enhances the AI's ability to understand and respond to specific topics or queries relevant to the embedded data. Key benefits include:
      </p>
      <ul>
        <li><strong>Improved Accuracy:</strong> Tailored responses based on specific, contextual data.</li>
        <li><strong>Customization:</strong> Ability to fine-tune responses to fit unique use-cases or domains.</li>
        <li><strong>Context Awareness:</strong> Enhanced understanding of the nuances and specifics of the embedded data set.</li>
      </ul>
    </div>

    <!-- Regular Non-Contextual Model -->
    <div class="col-md-6">
      <h2>Regular Non-Contextual Model</h2>
      <p>
        In contrast, a regular non-contextual model lacks the ability to incorporate specific external data, relying solely on its pre-trained knowledge base. This can limit its effectiveness in certain scenarios. Key limitations include:
      </p>
      <ul>
        <li><strong>Generic Responses:</strong> Tends to provide more general answers that might not align with specific user needs.</li>
        <li><strong>Limited Customization:</strong> Less flexibility in tailoring the model to specific domains or datasets.</li>
        <li><strong>Lack of Depth:</strong> May not fully grasp or respond accurately to niche or specialized queries.</li>
      </ul>
    </div>
  </div>
  <p>Congratulations, you've used custom web scraped data and embedded it into a conversational model!</p>
</div>
</div>
      
</main>

      <footer class="bg-light text-center text-lg-start">
        <!-- Copyright -->
        <div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0.2);">
          © 2023 Copyright:
          <a class="text-dark" href="https://ileka2468.github.io/website">William Ileka</a>
        </div>
      </footer>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script type="module" src="../prism.js"></script>
</body>
</html>